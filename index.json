[{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/","section":"Darian's Blog","summary":"","title":"Darian's Blog"},{"content":"","date":null,"permalink":"/categories/lab-meetings/","section":"Categories","summary":"","title":"Lab Meetings"},{"content":"Make Code Run Fast # Here I provide some tips on how to make code run fast.\nProfile your code # Code profiling is a method that is used to detect how long each function or line of code takes to run and how often it gets executed.\nTiming code # In Jupyter Notebook we can use magic commands %%time and %timeit.\nimport time %%time time.sleep(1) CPU times: user 595 Âµs, sys: 1.02 ms, total: 1.61 ms Wall time: 1 s %timeit time.sleep(1) 1 s Â± 1.73 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) Customize the number of runs with -r and the number of loops with -n.\n%timeit -n2 -r2 time.sleep(1) 1 s Â± 128 Âµs per loop (mean Â± std. dev. of 2 runs, 2 loops each) You can also use the module timeit inside a python script.\nimport timeit timeit.timeit(\u0026#39;time.sleep(0.1)\u0026#39;, number=4) a = 1 timeit.timeit(\u0026#34;a+a\u0026#34;, number=10000, globals=globals()) 0.000622792000285699 Profiling code # For complicated algorithm or code, it is better to use a profiler. The profiler will give you a more detailed report of the code execution.\nRprof and profvis for profiling R code # Rprof is a built-in profiler in R. It can be used to profile R code. The profvis package provides a visual interface for Rprof. These tools works better in RStudio.\n%%writefile profile_example.R profvis::profvis({ foo1 \u0026lt;- function(n){ return(sum((0:(n-1))^2)) } foo2 \u0026lt;- function(n){ ans \u0026lt;- 0 for (i in 0:(n-1)) { ans \u0026lt;- ans + i^2 } } foo3 \u0026lt;- function(n){ for (i in 1:10) { foo1(n) } foo2(n) } foo4 \u0026lt;- function(n){ for (i in 1:100) { foo2(n) } } work \u0026lt;- function(n){ foo1(n) foo2(n) foo3(n) foo4(n) } work(1e6) }) Overwriting profile_example.R cProfile for profiling Python code # cProfile is a profiler written in C for profiling Python code. The pstats moduleâ€™s Stats class has a variety of methods for manipulating and printing the data saved into a profile results file. The snakeviz package provides a visual interface for cProfile.\nimport numpy as np def foo1(n): return np.sum(np.square(np.arange(n))) def foo2(n): return sum(i*i for i in range(n)) def foo3(n): [foo1(n) for i in range(10)] foo2(n) def foo4(n): return [foo2(n) for i in range(100)] def work(n): foo1(n) foo2(n) foo3(n) foo4(n) import cProfile import pstats pr = cProfile.Profile() pr.enable() work(int(1e6)) pr.disable() pr.dump_stats(\u0026#39;profile.prof\u0026#39;) # save the profile ps = pstats.Stats(pr).strip_dirs().sort_stats(\u0026#34;cumtime\u0026#34;) ps.print_stats() 102000470 function calls in 12.240 seconds Ordered by: cumulative time ncalls tottime percall cumtime percall filename:lineno(function) 2 0.000 0.000 12.240 6.120 interactiveshell.py:3472(run_code) 2 0.000 0.000 12.240 6.120 {built-in method builtins.exec} 1 0.000 0.000 12.240 12.240 588885064.py:1(\u0026lt;module\u0026gt;) 1 0.000 0.000 12.240 12.240 2834638900.py:16(work) 102 0.000 0.000 12.219 0.120 2834638900.py:6(foo2) 102 4.787 0.047 12.219 0.120 {built-in method builtins.sum} 1 0.000 0.000 11.922 11.922 2834638900.py:13(foo4) 1 0.000 0.000 11.922 11.922 2834638900.py:14(\u0026lt;listcomp\u0026gt;) 102000102 7.432 0.000 7.432 0.000 2834638900.py:7(\u0026lt;genexpr\u0026gt;) 1 0.000 0.000 0.134 0.134 2834638900.py:9(foo3) 11 0.010 0.001 0.021 0.002 2834638900.py:3(foo1) 1 0.000 0.000 0.013 0.013 2834638900.py:10(\u0026lt;listcomp\u0026gt;) 11 0.006 0.001 0.006 0.001 {built-in method numpy.arange} 11 0.000 0.000 0.004 0.000 \u0026lt;__array_function__ internals\u0026gt;:177(sum) 11 0.000 0.000 0.004 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function} 11 0.000 0.000 0.004 0.000 fromnumeric.py:2188(sum) 11 0.000 0.000 0.004 0.000 fromnumeric.py:69(_wrapreduction) 11 0.004 0.000 0.004 0.000 {method 'reduce' of 'numpy.ufunc' objects} 2 0.000 0.000 0.000 0.000 codeop.py:117(__call__) 2 0.000 0.000 0.000 0.000 {built-in method builtins.compile} 2 0.000 0.000 0.000 0.000 contextlib.py:287(helper) 2 0.000 0.000 0.000 0.000 contextlib.py:104(__init__) 2 0.000 0.000 0.000 0.000 traitlets.py:692(__get__) 11 0.000 0.000 0.000 0.000 fromnumeric.py:70(\u0026lt;dictcomp\u0026gt;) 2 0.000 0.000 0.000 0.000 contextlib.py:141(__exit__) 4 0.000 0.000 0.000 0.000 {built-in method builtins.next} 2 0.000 0.000 0.000 0.000 contextlib.py:132(__enter__) 2 0.000 0.000 0.000 0.000 interactiveshell.py:1229(user_global_ns) 4 0.000 0.000 0.000 0.000 compilerop.py:180(extra_flags) 2 0.000 0.000 0.000 0.000 interactiveshell.py:3424(compare) 4 0.000 0.000 0.000 0.000 {built-in method builtins.getattr} 2 0.000 0.000 0.000 0.000 traitlets.py:654(get) 11 0.000 0.000 0.000 0.000 {built-in method builtins.isinstance} 11 0.000 0.000 0.000 0.000 fromnumeric.py:2183(_sum_dispatcher) 11 0.000 0.000 0.000 0.000 {method 'items' of 'dict' objects} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} \u0026lt;pstats.Stats at 0x10c8c0fd0\u0026gt; In shell run:\nsnakeviz profile.prof BLAS and LAPACK # BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) are specifications that prescribe a set of low-level routines for performing common linear algebra operations They are used by many other softwares and packages, such as R, NumPy, SciPy, and scikit-learn.\nfrom threadpoolctl import threadpool_limits, threadpool_info,ThreadpoolController This numpy installed via pip by default rely on the OpenBLAS, one of the implementations of BLAS and LAPACK. Other implementations include Intel MKL and Apple Accelerate, etc.\nThe M2 chip has 8 cores and 8 threads. Half of the cores are performance cores and the other half are efficiency cores. 12-th and 13-th generation Intel chips also have performance cores and efficiency cores. Most AMD and Intel CPUs use hyperthreading to allow a single core to execute two threads simultaneously. The number of threads code use affect the performance a lot.\nthreadpool_info() [{'user_api': 'blas', 'internal_api': 'openblas', 'prefix': 'libopenblas', 'filepath': '/Users/dawang/Projects/lab_meeting/.venv/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib', 'version': '0.3.21', 'threading_layer': 'pthreads', 'architecture': 'armv8', 'num_threads': 8}] def mat_mul_8T(n): A = np.random.rand(100,100) B = np.random.rand(100,100) for i in range(n): np.matmul(A,B) %timeit mat_mul_8T(10000) 730 ms Â± 90.5 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) controller = ThreadpoolController() @controller.wrap(limits=4,user_api=\u0026#39;blas\u0026#39;) def mat_mul_4T(n): A = np.random.rand(100,100) B = np.random.rand(100,100) for i in range(n): np.matmul(A,B) %timeit mat_mul_4T(10000) 173 ms Â± 517 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each) When using 4 threads, the matrix multiplication is about 3$\\times$ faster than using all 8 threads. The reason is that limiting the number of threads allows the M2 CPU to use the performance cores only, instead of waiting for the slow efficiency cores.\nFor AMD and Intel hyperthreading CPUs, using one thread per physical performance core is usually the fastest.\ncontroller = ThreadpoolController() @controller.wrap(limits=1,user_api=\u0026#39;blas\u0026#39;) def mat_mul_1T(n): A = np.random.rand(100,100) B = np.random.rand(100,100) for i in range(n): np.matmul(A,B) %timeit mat_mul_1T(10000) 482 ms Â± 2.68 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) Even turning off the multithreading for BLAS by limiting the number of thread to be 1 results in a speedup compared to using all 8 threads.\nChoosing the right BLAS and LAPACK implementation is also important. For example, the Intel MKL on Intel CPUs is usually faster than OpenBLAS. The Apple Accelerate on M1 and M2 chips is usually also faster than OpenBLAS.\nIn R, it is also possible to use alternative BLAS and LAPACK implementations. Please refer to the following links for more details.\nShort tutorial for setting up OpenBLAS in R for Windows\nUsing R with BLAS and LAPACK on WSL\nR package flexiblas for unix-like systems\nNumba and Cython # Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\nThe first example of computing $\\pi$ using Monte Carlo comes from the website of Numba.\nfrom numba import njit import random def mc_pi(nsamples): acc = 0 for i in range(nsamples): x = random.random() y = random.random() if (x ** 2 + y ** 2) \u0026lt; 1.0: acc += 1 return 4.0 * acc / nsamples @njit def mc_pi_jit(nsamples): acc = 0 for i in range(nsamples): x = random.random() y = random.random() if (x ** 2 + y ** 2) \u0026lt; 1.0: acc += 1 return 4.0 * acc / nsamples %timeit mc_pi(1000000) 110 ms Â± 2 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) %timeit mc_pi_jit(1000000) 5.79 ms Â± 2.82 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) The second example of Mandelbrot set comes from the documentation of Numba.\nPure python code is as follows:\n%matplotlib inline import matplotlib.pyplot as plt # color function for point at (x, y) def mandel(x, y, max_iters): c = complex(x, y) z = 0.0j for i in range(max_iters): z = z*z + c if z.real*z.real + z.imag*z.imag \u0026gt;= 4: return i return max_iters def create_fractal(xmin, xmax, ymin, ymax, image, iters): height, width = image.shape pixel_size_x = (xmax - xmin)/width pixel_size_y = (ymax - ymin)/height for x in range(width): real = xmin + x*pixel_size_x for y in range(height): imag = ymin + y*pixel_size_y color = mandel(real, imag, iters) image[y, x] = color gimage = np.zeros((1024, 1536), dtype=np.uint8) xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype(\u0026#39;float32\u0026#39;) iters = 50 %timeit -r1 -n1 create_fractal(xmin, xmax, ymin, ymax, gimage, iters) plt.grid(False) plt.imshow(gimage, cmap=\u0026#39;jet\u0026#39;) pass 3.27 s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each) from numba import uint32, float32 Specifying the data type in a Numba function can speed up the code a lot.\nmandel_numba = njit(uint32(float32, float32, uint32))(mandel) @njit def create_fractal_numba(xmin, xmax, ymin, ymax, image, iters): height, width = image.shape pixel_size_x = (xmax - xmin)/width pixel_size_y = (ymax - ymin)/height for x in range(width): real = xmin + x*pixel_size_x for y in range(height): imag = ymin + y*pixel_size_y color = mandel_numba(real, imag, iters) image[y, x] = color gimage = np.zeros((1024, 1536), dtype=np.uint8) xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype(\u0026#39;float32\u0026#39;) iters = 50 %timeit -r1 -n1 create_fractal_numba(xmin, xmax, ymin, ymax, gimage, iters) plt.grid(False) plt.imshow(gimage, cmap=\u0026#39;jet\u0026#39;) 115 ms Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each) Cython is a superset of Python that compiles to C. It is a static compiler that requires type declarations. It is more difficult to use than Numba.\nUse built-in functions and proper libraries # We use an example of calculating the pairwise distance between a set of vectors to demonstrate the importance of using built-in functions.\nn = 100 p = 100 xs = np.random.random((n, p)) The most straightforward way is to use a nested loop. This is the slowest way.\ndef pdist_py(xs): \u0026#34;\u0026#34;\u0026#34;Unvectorized Python.\u0026#34;\u0026#34;\u0026#34; n, p = xs.shape A = np.zeros((n, n)) for i in range(n): for j in range(n): for k in range(p): A[i,j] += (xs[i, k] - xs[j, k])**2 A[i,j] = np.sqrt(A[i,j]) return A %timeit -r3 -n3 pdist_py(xs) 288 ms Â± 7.94 ms per loop (mean Â± std. dev. of 3 runs, 3 loops each) Since the pairwise distance is a symmetric matrix, we can only calculate the upper triangular part of the matrix and then copy the upper triangular part to the lower triangular part. This result in a 2$\\times$ speedup.\ndef pdist_py_sym(xs): \u0026#34;\u0026#34;\u0026#34;Unvectorized Python exploiting symmetry.\u0026#34;\u0026#34;\u0026#34; n, p = xs.shape A = np.zeros((n, n)) for i in range(n): for j in range(i+1,n): for k in range(p): A[i,j] += (xs[i, k] - xs[j, k])**2 A[i,j] = np.sqrt(A[i,j]) A += A.T return A %timeit -r3 -n3 pdist_py_sym(xs) 142 ms Â± 1.99 ms per loop (mean Â± std. dev. of 3 runs, 3 loops each) The distance between two vectors can be calculated by the numpy.linalg.norm function. This result in a large speedup.\ndef pdist_vec(xs): \u0026#34;\u0026#34;\u0026#34;Vectorized inner loop using numpy.\u0026#34;\u0026#34;\u0026#34; n, p = xs.shape A = np.zeros((n, n)) for i in range(n): for j in range(i+1,n): A[i,j] = np.linalg.norm(xs[i,:] - xs[j,:]) A += A.T return A %timeit -r3 -n3 pdist_vec(xs) 7.45 ms Â± 314 Âµs per loop (mean Â± std. dev. of 3 runs, 3 loops each) Further more, the two-layer loop can be replaced by a numpy broadcasting operation.\ndef pdist_np(xs): \u0026#34;\u0026#34;\u0026#34;Fully vectorized using numpy.\u0026#34;\u0026#34;\u0026#34; return np.sqrt(np.sum((xs[:,None,:] - xs[None,:,:])**2, axis=-1)) %timeit -r30 -n30 pdist_np(xs) 1.25 ms Â± 48.2 Âµs per loop (mean Â± std. dev. of 30 runs, 30 loops each) Also, we can use the JIT techneque to speed up the code.\npdist_numba = njit(pdist_py_sym, cache = True) %timeit -r30 -n30 pdist_numba(xs) The slowest run took 15.81 times longer than the fastest. This could mean that an intermediate result is being cached. 428 Âµs Â± 757 Âµs per loop (mean Â± std. dev. of 30 runs, 30 loops each) However, the fastest way is to use the scipy.spatial.distance.pdist function.\nfrom scipy.spatial.distance import pdist, squareform %timeit -r30 -n30 squareform(pdist(xs)) 128 Âµs Â± 3.45 Âµs per loop (mean Â± std. dev. of 30 runs, 30 loops each) Let\u0026rsquo;s check whether the results are the same.\nnp.allclose(pdist_np(xs), squareform(pdist(xs))) True Another example is to calculate the matrix multiplication. The numpy.dot function and the @ operator are most frequently used. But sometimes the torch library is faster.\nimport torch A = np.random.random((1000, 1000)) B = np.random.random((1000, 1000)) %timeit -r10 -n10 np.dot(A, B) %timeit -r10 -n10 A @ B %timeit -r10 -n10 np.matmul(A, B) 23.7 ms Â± 2.65 ms per loop (mean Â± std. dev. of 10 runs, 10 loops each) 24.2 ms Â± 2.54 ms per loop (mean Â± std. dev. of 10 runs, 10 loops each) 23.9 ms Â± 1.91 ms per loop (mean Â± std. dev. of 10 runs, 10 loops each) torch_A = torch.from_numpy(A) torch_B = torch.from_numpy(B) %timeit -r10 -n10 torch.matmul(torch_A, torch_B) 8.07 ms Â± 2.26 ms per loop (mean Â± std. dev. of 10 runs, 10 loops each) cpp in Python and R # Todo\u0026hellip;\nSources # pybind11\ncppimport\nEigen\nArmadillo\nRcpp\nRcppArmadillo\nRcppEigen\nParallel computing # Todo\u0026hellip;\n","date":"1 June 2023","permalink":"/posts/make_code_run_fast/","section":"Posts","summary":"Here I provide some tips on how to make code run fast.","title":"Make Code Run Fast"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/programming/","section":"Tags","summary":"","title":"Programming"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"è¿™å‡ å¤©æŠŠä¸ªäººåšå®¢æ­äº†èµ·æ¥ï¼Œè®°å½•ä¸€ä¸‹è¿‡ç¨‹ã€‚\nèµ·å›  # å…¶å®ä¸€ç›´æƒ³æ­ä¸€ä¸ªåšå®¢ç©ç©ï¼Œä¹‹å‰å…¶å®ä¹Ÿæœ‰è¯•è¿‡ç”¨ hexo ï¼Œå¹¶ä¸”éƒ¨ç½²åœ¨æ”¾åœ¨å­¦æ ¡çš„ä¸€å°ç”µè„‘ä¸Šï¼Œä½†æ²¡æœ‰å›ºå®š ip å¹¶ä¸”åªä¸ºäº†ä¸€ä¸ªé™æ€ç½‘ç«™ä¸€ç›´å¼€ç€ç”µè„‘ä¹Ÿæœ‰ç‚¹æµªè´¹ï¼ˆå…¶å®å¹¶ä¸ï¼Œæ˜¯ä¸€å°è£…äº† pve çš„ä¸»æœºï¼Œè¿˜è¿è¡Œç€ nas ç­‰æœåŠ¡ï¼‰ã€‚ä¸»è¦åŸå› è¿˜æ˜¯æ²¡ä»€ä¹ˆä¸œè¥¿å¯å†™ï¼Œæ‰€ä»¥ä¸Šä¸€ä¸ªåŸŸåè¿‡æœŸåå°±ä¸äº†äº†ä¹‹äº†ã€‚\nå‰æ®µæ—¶é—´ç”¨å­¦ç”Ÿé‚®ç®±ç”³è¯·äº†ä¸€ä¸ªå¾®è½¯çš„ Azure å…è´¹è´¦æˆ·ï¼Œä¸»è¦æ˜¯æƒ³å…è´¹ç”¨å®ƒæä¾›çš„è™šæ‹Ÿæœºã€‚ï¼ˆå¾®è½¯å¯¹å¤§é™†çš„ .edu.cn é‚®ç®±å¥½åƒéå¸¸ä¸¥æ ¼ï¼Œå°è¯•ç›´æ¥éªŒè¯å’Œ GitHub student developer pack è®¤è¯éƒ½ä¸è¡Œï¼Œæœ€åå’Œå®¢æœé‚®ä»¶+ç”µè¯æ‰¯äº†ä¸€å‘¨æ‰æ³¨å†ŒæˆåŠŸã€‚ï¼‰ç„¶åå¶ç„¶çœ‹åˆ° Azure ä¹Ÿå¯ä»¥éƒ¨ç½² static web apps ï¼Œå°±æƒ³è¯•ç”¨ä¸€ä¸‹ã€‚å½“ç„¶è¿˜æœ‰å¾ˆå¤šç±»ä¼¼çš„æœåŠ¡æä¾›å•†ï¼Œæ¯”å¦‚ GitHub pages, Vercel, Netlifyç­‰ç­‰ã€‚\nåŸŸå # åŒæ ·æ˜¯ç”¨å­¦ç”Ÿé‚®ç®±åœ¨ Namecheap æ³¨å†Œçš„å…è´¹ .me åŸŸåï¼Œä¸è¿‡ä¸€å¹´ååº”è¯¥å°±è¦ç»­è´¹äº†ã€‚\nç½‘ç«™ç”Ÿæˆå·¥å…·ï¼šHugo # ","date":"19 April 2023","permalink":"/posts/building_the_blog/","section":"Posts","summary":"","title":"è®°å½•ä¸ªäººåšå®¢æ­å»ºï¼šä½¿ç”¨Hugoä»¥åŠCongoä¸»é¢˜"},{"content":" æš‚æ—¶åªé…ç½®äº† Canokey çš„ PGP å¯†é’¥ï¼ŒåŒ…æ‹¬ç­¾åã€è®¤è¯å’ŒåŠ å¯†ã€‚å…¶ä¸­ç­¾åå¯†é’¥ç”¨æ¥ç»™ git çš„commitç­¾åã€‚ æµ‹è¯•ä¸€ä¸‹ GitHub æ–°çš„commitæ˜¯å¦æ˜¾ç¤º verifiedã€‚\nä½ å¯ä»¥åœ¨ è¿™é‡Œ æ‰¾åˆ°æˆ‘çš„PGPå…¬é’¥ã€‚\ngpg -expert --full-gen-key gpg --quick-add-key 1548C03A3741D936 cv25519 encr gpg --quick-add-key 1548C03A3741D936 ed25519 sign gpg --quick-add-key 1548C03A3741D936 ed25519 auth gpg -ao ppg-key.pub --export 1548C03A3741D936 gpg -ao sec-key.asc --export-private-key 1548C03A3741D936! gpg -ao encr-key.asc --export-private-key \u0026lt;fingerprint\u0026gt;! gpg -ao sign-key.asc --export-private-key \u0026lt;fingerprint\u0026gt;! gpg -ao auth-key.asc --export-private-key \u0026lt;fingerprint\u0026gt;! gpg --edit-key 1548C03A3741D936 gpg/key\u0026gt; 1 gpg/key\u0026gt; keytocard gpg --edit-card gpg/card\u0026gt; fetch git config --global user.signingkey 1548C03A3741D936 git commit -S -m \u0026#34;This commit is signed with my PGP key.\u0026#34; ","date":"18 April 2023","permalink":"/posts/learning_to_use_gnugpg_and_canokey/","section":"Posts","summary":"ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æˆ‘çš„PGPå…¬é’¥ã€‚","title":"ä½¿ç”¨ PGP å’Œ Canokey"},{"content":" Darian Wang (ç‹è¾¾) # I am a PhD student at Center for Statistical Sciences, Tsinghua University.\nMy PGP key fingerprint is FB95 1601 3547 40FD 3BFF FA54 18DA FD16 E108 DDC9.\nMy research interests include\nStatistical computing Bayesian statistics Markov Chain Monte Carlo Statistical Machine Learning False Discovery Rate Education # Ph.D Statistics, Tsinghua University, 2020-now B.S. Mathematics, Tsinghua University, 2016-2020 Publications # Errr\u0026hellip;.. No publications yet.\nTeaching # 2020 Fall: Statistical Inference (undergraduate, TA) 2020 Spring: Introduction to Statistics: the Science and Art of Data Analysis (undergraduate, TA) 2021 Fall: Statistical Inference (undergraduate, TA) 2021 Spring: Advanced Statistical Computing (graduate, TA) ","date":"15 April 2023","permalink":"/about/","section":"Darian's Blog","summary":"Darian Wang (ç‹è¾¾) # I am a PhD student at Center for Statistical Sciences, Tsinghua University.","title":"About Me"},{"content":"","date":null,"permalink":"/categories/development/","section":"Categories","summary":"","title":"Development"},{"content":"","date":null,"permalink":"/categories/test/","section":"Categories","summary":"","title":"test"},{"content":"","date":null,"permalink":"/tags/%E5%8D%9A%E5%AE%A2/","section":"Tags","summary":"","title":"åšå®¢"},{"content":"","date":null,"permalink":"/tags/%E6%B5%8B%E8%AF%95%E7%94%A8/","section":"Tags","summary":"","title":"æµ‹è¯•ç”¨"},{"content":"This post is a test post.\nHAHA # æµ‹è¯• # 3rd level # 4th level # 5th level # æµ‹è¯•ä¸€ä¸‹å“¦ æµ‹è¯•ä¸€ä¸‹å“¦ New article! Call to action \\(f(a,b,c) = (a^2+b^2+c^2)^3\\)\nTest Test test \\(\\alpha\\) $$ \\beta $$ [1]/[2]\nWarning! This action is destructive! def add(a,b): return a+b ğŸ˜‚\n","date":"15 April 2023","permalink":"/posts/first_post/","section":"Posts","summary":"æµ‹è¯•summary","title":"ç¬¬ä¸€ç¯‡æµ‹è¯•"}]